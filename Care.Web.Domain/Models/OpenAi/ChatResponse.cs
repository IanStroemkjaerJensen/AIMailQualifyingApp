using System.Text.Json.Serialization;

namespace Care.Web.Domain.Models.OpenAi;

public class ChatResponse
{
    /// <summary>
    /// Id: A unique identifier for the chat completion.
    /// </summary>
    [JsonPropertyName("id")]
    public string Id { get; set; }

    /// <summary>
    /// Object: The object type, which is always chat.completion
    /// </summary>
    [JsonPropertyName("object")]
    public string? Object { get; set; }

    /// <summary>
    /// Created: The Unix timestamp (in seconds) of when the chat completion was created
    /// </summary>
    [JsonPropertyName("created")]
    private int _created;

    /// <summary>
    /// Created: When the chat completion was created
    /// </summary>
    public DateTime Created
    {
        get => DateTimeOffset.FromUnixTimeSeconds(_created).DateTime;
        set => _created = (int)((DateTimeOffset)value).ToUnixTimeSeconds();
    } // TODO: Virker ikke

    /// <summary>
    /// SystemFingerprint: This fingerprint represents the backend configuration that the model runs with.
    /// Can be used in conjunction with the seed request parameter to understand when backend changes have been made that might impact determinism
    /// </summary>
    [JsonPropertyName("system_fingerprint")]
    public string? SystemFingerprint { get; set; }

    /// <summary>
    /// Model: The model used for the chat completion
    /// </summary>
    [JsonPropertyName("model")]
    public string Model { get; set; }

    /// <summary>
    /// Choices: A list of chat completion choices. Can be more than one if n is greater than 1
    /// </summary>
    [JsonPropertyName("choices")]
    public Choice[] Choices { get; set; }

    /// <summary>
    /// Usage: Usage statistics for the completion request
    /// </summary>
    [JsonPropertyName("usage")]
    public Usage Usage { get; set; }
}

public class Usage
{
    /// <summary>
    /// PromptTokens: Number of tokens in the prompt
    /// </summary>
    [JsonPropertyName("prompt_tokens")]
    public int PromptTokens { get; set; }

    /// <summary>
    /// CompletionTokens: Number of tokens in the generated completion
    /// </summary>
    [JsonPropertyName("completion_tokens")]
    public int CompletionTokens { get; set; }

    /// <summary>
    /// TotalTokens: Total number of tokens used in the request (prompt + completion)
    /// </summary>
    [JsonPropertyName("total_tokens")]
    public int TotalTokens { get; set; }
}

public class Choice
{
    /// <summary>
    /// Index: The index of the choice in the list of choices
    /// </summary>
    [JsonPropertyName("index")]
    public int Index { get; set; }

    /// <summary>
    /// FinishReason: The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, 
    /// length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters,
    /// tool_calls if the model called a tool, or function_call (deprecated) if the model called a function.
    /// </summary>
    [JsonPropertyName("finish_reason")]
    [JsonConverter(typeof(JsonStringEnumConverter))]
    public FinishReason FinishReason { get; set; }

    /// <summary>
    /// Message: A chat completion message generated by the model
    /// </summary>
    [JsonPropertyName("message")]
    public Message Message { get; set; }
}

public enum FinishReason
{
    /// <summary>
    /// Stop: API returned complete message, or a message terminated by one of the stop sequences provided via the stop parameter
    /// </summary>
    Stop,
    /// <summary>
    /// Length: Incomplete model output due to max_tokens parameter or token limit
    /// </summary>
    Length,
    /// <summary>
    /// function_call: The model decided to call a function
    /// </summary>
    Function_call,
    /// <summary>
    ///  content_filter: Omitted content due to a flag from our content filters
    /// </summary>
    Content_filter,
    /// <summary>
    /// null: API response still in progress or incomplete
    /// </summary>
    Null

}